{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrF-VMY-iMqp",
        "outputId": "c7339a23-e6f7-4ad1-851b-163012d830bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "def summarize_text(text, compression_ratio=0.4):\n",
        "    # Step 1: Tokenize the text into words and sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Step 2: Remove stopwords and build a frequency table\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_words = [w for w in words if w.isalnum() and w not in stop_words]\n",
        "\n",
        "    freq_table = defaultdict(int)\n",
        "    for word in filtered_words:\n",
        "        freq_table[word] += 1\n",
        "\n",
        "    # Normalize frequency (ensures fairness across large texts)\n",
        "    max_freq = max(freq_table.values())\n",
        "    for word in freq_table:\n",
        "        freq_table[word] /= max_freq\n",
        "\n",
        "    # Step 3: Score each sentence using normalized word frequencies\n",
        "    sentence_scores = defaultdict(float)\n",
        "    for sentence in sentences:\n",
        "        sentence_words = word_tokenize(sentence.lower())\n",
        "        for word in sentence_words:\n",
        "            if word in freq_table:\n",
        "                sentence_scores[sentence] += freq_table[word]\n",
        "\n",
        "        # Length normalization encourages concise sentences\n",
        "        sentence_scores[sentence] /= (len(sentence_words) + 1)\n",
        "\n",
        "    # Step 4: Select top sentences (based on compression ratio)\n",
        "    sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "    summary_count = max(1, int(len(sentences) * compression_ratio))\n",
        "    summary = \" \".join(sorted_sentences[:summary_count])\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# SAMPLE USAGE\n",
        "text = \"\"\"Text summarization is a crucial process in an age of information overload, designed to distill extensive articles, reports, or documents into a concise, manageable overview while preserving the original content's core meaning and essential information. This process can be performed manually or through advanced AI-powered tools that use algorithms to identify and extract the most relevant sentences (extractive summarization) or generate new, human-like sentences to cover the main ideas (abstractive summarization). The primary goal is to save time, enhance understanding, and quickly determine whether the full text is worth reading, which is especially beneficial for students, researchers, and professionals reviewing large volumes of information. Key components of a good summary include clarity, brevity, objectivity (avoiding personal opinions or analysis), and accuracy, focusing solely on the main topic, purpose, and key supporting details while omitting minor details, descriptive language, or direct quotations (unless cited). Users can often customize the output to be a single paragraph or a list of bullet points, and adjust the length to suit different needs, from a brief overview to a more detailed executive summary. A variety of online platforms, such as QuillBot, Scribbr, and Grammarly, offer free and premium summarization tools to help with this task.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize_text(text)\n",
        "print(\"\\n--- SUMMARY ---\\n\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e2LZ79NiRcG",
        "outputId": "fef4f14e-c75e-4b6c-f578-f88336fd5c2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- SUMMARY ---\n",
            " Text summarization is a crucial process in an age of information overload, designed to distill extensive articles, reports, or documents into a concise, manageable overview while preserving the original content's core meaning and essential information. This process can be performed manually or through advanced AI-powered tools that use algorithms to identify and extract the most relevant sentences (extractive summarization) or generate new, human-like sentences to cover the main ideas (abstractive summarization).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbudxgWvid7O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}